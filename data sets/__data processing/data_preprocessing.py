'''
CODE FOR PREPROCESSING AND DISCRETIZING DATA SETS. 
The pipeline uses a .json file in the format generated by 
the notebook "data_analysis.ipynb"

This script process the entire < EsmamDS/data sets/_original data sets > folder
and saves the processed data sets into < EsmamDS/data sets/final data sets >

Alternativelly, to process a single data set use --db 
providing the data set name (must be a file in <_original data sets> folder)
'''

import argparse
import errno
import glob
import json
import numpy as np
import os
import pandas as pd

from sklearn.preprocessing import KBinsDiscretizer

# global variables
VAR_TIME_NAME = 'survival_time'
VAR_EVENT_NAME = 'survival_status'
N_BINS = 5
ENCODE = 'ordinal'
STRATEGY = 'quantile'

# paths
ROOT = "EsmamDS"
PATH = os.path.dirname(__file__).split(ROOT)[0]+ROOT # read system path up to ROOT
DATA_PATH = PATH+"\\data sets\\"
ORIG_DATA_PATH = DATA_PATH+"_original data sets\\"
FINAL_DATA_PATH = DATA_PATH+"final data sets\\"


def _save_db(_db, _db_name):
    
    with open(FINAL_DATA_PATH+'{}_dtypes.json'.format(_db_name), 'w') as f:
        json.dump(_db.dtypes.apply(lambda x: x.name).to_dict(),f)
    
    _db.to_csv(FINAL_DATA_PATH+'{}.xz'.format(_db_name), index=False, compression='xz')
    print('>> saved '+_db_name)
    return


def preprocessing(_data_path, json_obj, _discretization=True, _db_name=''):
    
    # read db with [adjusted missing values]
    if json_obj['_dictMV']:
        mv_values = set([item for l in json_obj['_dictMV'].values() for item in l])
        db = pd.read_csv(_data_path, header=0, sep=',', na_values=mv_values)
    else:
        db = pd.read_csv(_data_path, header=0, sep=',')
    
    # drop columns
    cols2drop = json_obj['_emptCols'] + json_obj['_colsNot2use']
    if cols2drop:
        db.drop(columns=cols2drop, inplace=True)
    
    # rename survival attributes
    surv2change = {json_obj['_survivalAttr']['survivalTime_name']: VAR_TIME_NAME,
                   json_obj['_survivalAttr']['survivalEvent_name']: VAR_EVENT_NAME}
    db.rename(columns=surv2change, inplace=True)
    
    # adjust event variable
    if db[VAR_EVENT_NAME].isna().sum():                                                  # drop missing values on event feature
        db.dropna(subset=[VAR_EVENT_NAME], inplace=True)
        db.reset_index(drop=True, inplace=True)
    if (json_obj['_statusRepr']['censValue'] and json_obj['_statusRepr']['eventValue']): # if event representation adjustment
        repr2change = {VAR_EVENT_NAME:{}}
        for value in json_obj['_statusRepr']['censValue']:
            repr2change[VAR_EVENT_NAME][value] = 'False'
        for value in json_obj['_statusRepr']['eventValue']:
            repr2change[VAR_EVENT_NAME][value] = 'True'
        db.replace(to_replace=repr2change, inplace=True)      
        mask = {'False': 0, 'True': 1}
        db[VAR_EVENT_NAME].replace(to_replace=mask, inplace=True)
    
    # input categories
    if json_obj['_nan2replace']:
        db.fillna(value=json_obj['_nan2replace'], inplace=True)
    
    # dropping missing values
    db.dropna(inplace=True, how='any')
    db.reset_index(drop=True, inplace=True)
    
    # adjust dtypes
    dtypes = {}.fromkeys(json_obj['_colsType'].keys(), 'category')
    if json_obj['_survivalAttr']['survivalEvent_name'] in dtypes:                       # remove old VAR_EVENT_NAME
        del dtypes[json_obj['_survivalAttr']['survivalEvent_name']]                     # remove old VAR_TIME_NAME
    if json_obj['_survivalAttr']['survivalTime_name'] in dtypes:
        del dtypes[json_obj['_survivalAttr']['survivalTime_name']]
    dtypes[VAR_EVENT_NAME] = 'bool'                                        # add VAR_EVENT_NAME as bool
    numerical = dict.fromkeys(list(set(dtypes.keys()) ^ set(db.columns)),'int64')
    db = db.astype({**dtypes,**numerical}, copy=True)
    
    # reorder columns with survival features first
    cols = list(db.columns)
    cols.remove(VAR_EVENT_NAME)
    cols.remove(VAR_TIME_NAME)
    cols = [VAR_TIME_NAME, VAR_EVENT_NAME] + cols
    db = db.reindex(columns=cols)
    
    # discretization
    if _discretization:
        db = discretization(db, json_obj, _db_name)
    
    # save processed database
    _save_db(db, _db_name)
    return


def discretization(_database, json_obj, _db_name):
    
    LOG_FILE = DATA_PATH+"__data processing\\_disc logs\\{}_log_discretization.json".format(_db_name)
    
    # remove old VAR_TIME_NAME from features to discretize
    if json_obj['_survivalAttr']['survivalTime_name'] in json_obj['_cols2disc']:
        json_obj['_cols2disc'].remove(json_obj['_survivalAttr']['survivalTime_name'])
        
    # no feature for discretisation
    if not json_obj['_cols2disc']:
        # save log discretization
        with open(LOG_FILE, 'w') as f:
            json.dump('! This data set has no features for discretization',f)
        return _database
    
    # discretization process: iterates over all coluns
    log_discretization = {}
    db = _database.copy()
    db_disc = db.copy()
    
    for col_name in json_obj['_cols2disc']:
        col_log = {}
        
        # data shape to discretize
        data = np.array(db[col_name]).reshape(-1, 1)
        
        discretizer = KBinsDiscretizer(n_bins=N_BINS, encode=ENCODE, strategy=STRATEGY)
        discretizer.fit(data)
        # data discretized/encoded
        data_encoded = pd.Series(discretizer.transform(data).reshape(1,-1)[0]).astype('int64')
        
        # categories representation
        categories = sorted(data_encoded.unique().tolist())
        bin_edges = list(discretizer.bin_edges_[0])
        map_names = {}
        for idx,ctg in enumerate(categories):
            if idx+1 == len(categories):
                string = '[{:0.2f},{:0.2f}]'.format(bin_edges[idx],bin_edges[idx+1])
            else:
                string = '[{:0.2f},{:0.2f})'.format(bin_edges[idx],bin_edges[idx+1])
            map_names[ctg] = string
        
        # data decodification
        data_ctg = data_encoded.map(map_names)
        db_disc[col_name] = data_ctg.astype('category')

        # register log for discretized column
        col_log['discretizer_params'] = discretizer.get_params().copy()
        col_log['bin_edges'] = bin_edges
        col_log['dict_categories_names'] = map_names
        col_log['data_orig'] = data.tolist()
        col_log['data_encoded'] = data_encoded.tolist()
        col_log['data_categories'] = data_ctg.tolist()
        log_discretization[col_name] = col_log.copy()
    
    # save log discretization
    with open(LOG_FILE, 'w') as f:
        json.dump(log_discretization,f)
        
    return db_disc


def process_file(db_name, disc):
    
    # prep file
    prep_file = DATA_PATH+"__data processing\\_prep files\\{}_prep.json".format(db_name)
    with open(prep_file, 'r') as f:
        json_obj = json.load(f)
        
    file_path = ORIG_DATA_PATH+"{}.csv".format(db_name)
    preprocessing(file_path, json_obj, _discretization=disc, _db_name=db_name)
    return


def process_folder(disc):
    
    files_path = ORIG_DATA_PATH+'*.csv'
    
    for file in glob.iglob(files_path):
        
        print('>> process file: {}'.format(file))
        db_name = file.split("\\")[-1].split('.')[0]
        process_file(db_name, disc)
    return

if __name__ == '__main__':
    
    # parse args setting
    parser = argparse.ArgumentParser(description='Script to perform data preprocessing and discretisation.')
    
    parser.add_argument("--db", type=str,
                        help="Data set (file) name to single process")
    parser.add_argument("--nodisc", default=True, action='store_false',
                        help="Do not perform data discretisation")
    
    args = parser.parse_args()
    
    # creates directory for saving results and logs
    if not os.path.exists(os.path.dirname(FINAL_DATA_PATH)):
        try:
            os.makedirs(os.path.dirname(FINAL_DATA_PATH))
        except OSError as exc:  # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise

    if args.db:
        process_file(args.db, args.nodisc)
        
    else:
        process_folder(args.nodisc)
    
    
